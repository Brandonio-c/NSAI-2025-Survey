"paper_id","title","cluster","Paper ID (from the excel spreadsheet) ","year"
"7","Knowledge Graph Completing with Dual Confrontation Learning Model based on Variational Information Bottleneck Method","0","7","2023"
"22","LLM Internal States Reveal Hallucination Risk Faced With a Query","2","22","2024"
"33","BertNet: Harvesting Knowledge Graphs with Arbitrary Relations from Pretrained Language Models","0","33","2023"
"47","Knowledge Neurons in Pretrained Transformers","4","47","2022"
"56","Journey to the Center of the Knowledge Neurons: Discoveries of Language-Independent Knowledge Neurons and Degenerate Knowledge Neurons","4","56","2023"
"57","Locating and Extracting Relational Concepts in Large Language Models","1","57","2024"
"61","InternalInspector I² Robust Confidence Estimation in LLMs through Internal States","2","61","2024"
"72","On Contrasting YAGO with GPT-J An Experiment for Person-Related Attributes","0","72","2022"
"74","LLMMaps – A Visual Metaphor for Stratified Evaluation of Large Language Models","5","74","2023"
"75","DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models","2","75","2023"
"77","Measuring the Knowledge Acquisition-Utilization Gap in Pre-trained Language Models","5","77","2023"
"81","Are Large Language Models More Honest in Their Probabilistic or Verbalized Confidence?","5","81","2024"
"84","PMET Precise Model Editing in a Transformer","4","84","2023"
"87","Self-Knowledge Guided Retrieval Augmentation for Large Language Models","5","87","2023"
"88","Large language models converge toward human-like concept organization","0","88","2024"
"97","Exploring In-Context Learning Capabilities of Foundation Models for Generating Knowledge Graphs from Text","0","97","2023"
"101","Cracking Factual Knowledge A Comprehensive Analysis of Degenerate Knowledge Neurons in Large Language Models","4","101","2024"
"108","Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models","2","108","2024"
"109","CN-AutoMIC Distilling Chinese Commonsense Knowledge from Pretrained Language Models","3","109","2022"
"111","LINEARITY OF RELATION DECODING IN TRANSFORMER LANGUAGE MODELS","1","111","2024"
"115","(COMET-) ATOMIC 2020: On Symbolic and Neural Commonsense Knowledge Graphs","3","115","2021"
"116","Dissecting Recall of Factual Associations in Auto-Regressive Language Models","1","116","2023"
"117","Combining prompt learning with contextual semantics for inductive relation prediction","0","117","2024"
"118","Probing Pretrained Language Models for Lexical Semantics","0","118","2020"
"122","Interpreting Language Models Through Knowledge Graph Extraction","0","122","2021"
"126","Improving Relation Extraction by Knowledge Representation Learning","3","126","2021"
"139","Interpretability of BERT Latent Space through Knowledge Graphs","0","139","2022"
"151","DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate Hallucinations","2","151","2024"
"152","Data-Efficient Concept Extraction from Pre-trained Language Models for Commonsense Explanation Generation","3","152","2022"
"154","Monitoring Latent World States in Language Models with Propositional Probes","1","154","2025"
"160","From Unstructured Data to Knowledge Graphs An Application for Compliance Checking Problem","0","160","2024"
"163","Crawling The Internal Knowledge-Base of Language Models","0","163","2023"
"164","Inspecting and Editing Knowledge Representations in Language Models","1","164","2023"
"165","LLMs Know More Than They Show On the Intrinsic Representation of LLM Hallucinations","2","165","2024"
"174","Knowledge Localization Mission Not Accomplished? Enter Query Localization!","4","174","2025"
"177","LlamaCare: A Large Medical Language Model for Enhancing Healthcare Knowledge Sharing","5","177","2024"
"190"," Pre-training Text-to-Text Transformers for Concept-Centric Common Sense","3","190","2023"
"191","Beneath the Surface of Consistency Exploring Cross-lingual Knowledge Representation Sharing in LLMs","0","191","2024"
"192","Identifying Query-Relevant Neurons in Large Language Models for Long-Form Texts","4","192","2025"
"193","Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation","5","193","2024"
"194","DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models","2","194","2023"
"196","Teaching Large Language Models to Express Knowledge Boundary from Their Own Signals","5","196","2024"
"197","Towards Automated Circuit Discovery for Mechanistic Interpretability","1","197","2023"
"198","Challenges with unsupervised LLM knowledge discovery","5","198","2023"
"199","Codebook Features: Sparse and Discrete Interpretability for Neural Networks","1","199","2023"
"199","Codebook Features: Sparse and Discrete Interpretability for Neural Networks","1","199","2024"
