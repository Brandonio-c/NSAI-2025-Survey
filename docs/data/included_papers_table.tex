% This table requires the longtable package.
% Add \usepackage{longtable} to your document preamble.
%
% All special LaTeX characters (including underscores) have been properly escaped.
%
\begin{longtable}{|p{2cm}|p{4cm}|p{8cm}|}
\hline
\textbf{Citation} & \textbf{Title} & \textbf{Description} \\
\hline
\endfirsthead

\hline
\textbf{Citation} & \textbf{Title} & \textbf{Description} \\
\hline
\endhead

\hline
\endfoot

\hline
\endlastfoot

    \cite{rayyan-242083818}\\\small\href{https://github.com/lotussavy/IWCMC-2025/tree/main}{Codebase} & \href{https://www.researchgate.net/publication/388686587_Neurosymbolic_AI_for_Travel_Demand_Prediction_Integrating_Decision_Tree_Rules_into_Neural_Networks}{Neurosymbolic AI for Travel Demand Prediction: Integrating Decision Tree Rules into Neural Networks} & This paper introduces a neuro-symbolic AI framework that blends decision tree rules with neural networks to predict travel demand. By combining interpretability with deep learning power, it achieves more accurate and explainable results for transportation planning and resource optimization. \\
    \cite{rayyan-242083826}\\\small\href{https://github.com/azreasoners/gpt-asp-rules?tab=readme-ov-file}{Codebase} & \href{https://doi.org/10.24963/kr.2023/37}{Leveraging Large Language Models to Generate Answer Set Programs} & They use LLMs to work through logic puzzle solving in a step-by-step manner. \\
    \cite{rayyan-242083828}\\\small\href{https://github.com/muraliadithya/vdp}{Codebase} & \href{https://arxiv.org/pdf/1907.05878}{Composing Neural Learning and Symbolic Reasoning with an Application to Visual Discrimination} & propose a compositional neurosymbolic framework that combines a neural network to detect objects and relationships with a symbolic learner that finds interpretable discriminators. \\
    \cite{rayyan-242083847}\\\small\href{https://github.com/KareemYousrii/SPL}{Codebase} & \href{https://proceedings.neurips.cc/paper_files/paper/2022/hash/c182ec594f38926b7fcb827635b9a8f4-Abstract-Conference.html}{Semantic probabilistic layers for neuro-symbolic learning} & The paper presents Semantic Probabilistic Layers (SPLs), a neural network module that ensures structured-output predictions are always consistent with logical constraints, enabling accurate and tractable neuro-symbolic learning. SPLs modularly combine probabilistic inference and logical reasoning, outperforming previous methods in tasks requiring strict output validity. \\
    \cite{rayyan-242083849}\\\small\href{https://github.com/UCLA-StarAI/NeSyEntropy}{Codebase} & \href{https://proceedings.mlr.press/v180/ahmed22a.html}{Neuro-symbolic entropy regularization} & This paper presents neuro-symbolic entropy regularization, a unified framework that combines entropy regularization and neuro-symbolic learning for structured prediction tasks. By constraining entropy minimization to outputs that form valid structures (as defined by logical circuits), the approach yields models that are both more accurate and more likely to produce valid predictions, demonstrated across semi-supervised and fully-supervised experiments. \\
    \cite{rayyan-242083862}\\\small\href{https://github.com/HannaAbiAkl/NeSy-Code-Generation-Workflow}{Codebase} & \href{https://doi.org/10.48550/arXiv.2402.16910}{NeSy is alive and well: A LLM-driven symbolic approach for better code comment data generation and classification} & This paper presents a neuro-symbolic workflow combining semantic rule–based decomposition with an LLM to generate controlled synthetic data for C-code comment classification. Empirical results show that this augmentation improves ML models (Voting Classifier, Random Forest, MLP) performance measured in F1 score. \\
    \cite{rayyan-242083875}\\\small\href{https://github.com/FutureComputing4AI/Hadamard-derived-Linear-Binding}{Codebase} & \href{https://arxiv.org/abs/2410.22669}{A Walsh Hadamard Derived Linear Vector Symbolic Architecture} & The paper introduces the Hadamard-derived Linear Binding (HLB), a novel vector symbolic architecture that leverages properties of the Walsh-Hadamard transform for efficient, numerically stable vector binding in neuro-symbolic AI. HLB achieves state-of-the-art performance on both classical VSA benchmarks and selected deep learning tasks, outperforming previous VSA methods in terms of computational complexity, accuracy, and differentiability for modern neural architectures. \\
    \cite{rayyan-242083885}\\\small\href{https://github.com/DanieleAlessandro/KENN2}{Codebase} & \href{https://doi.org/10.48550/arXiv.2205.15762}{Knowledge Enhanced Neural Networks for relational domains} & Paper extends knowledge enhanced NNs to handle relational data and shows that stacking multiple KE layers deals with rule dependencies, achieving better accuracy than baseline NNS on Citeseer citation network classification while also being faster than baseline Semantic Based Regularization (SBR) and Relational Neural Machines (RNM) \\
    \cite{rayyan-242083903}\\\small\href{https://github.com/Intelligent-CAT-Lab/AlphaTrans}{Codebase} & \href{https://arxiv.org/pdf/2410.24117}{AlphaTrans: A Neuro-Symbolic Compositional Approach for Repository-Level Code Translation and Validation} & Solves the task of code translation from one programming language to another. It does so using neural symbolic framework that breaks down the source code into fragments and also utilized the test code to ensure code was properly translated. \\
    \cite{rayyan-242083917}\\\small\href{https://github.com/neulab/retomaton}{Codebase} & \href{http://proceedings.mlr.press/v162/alon22a.html}{Neuro-symbolic language modeling with automaton-augmented retrieval} & The paper presents RETOMATON, a neuro-symbolic system that approximates costly nearest-neighbor datastore searches in retrieval-based language models using automaton states and pointer links, enabling substantial speed-ups while maintaining or improving perplexity. \\
    \cite{rayyan-242083986}\\\small\href{https://github.com/guicho271828/latplan}{Codebase} & \href{10.5555/3491440.3491811}{Learning neural-symbolic descriptive planning models via cube-space priors: The voyage home (to STRIPS)} & neuro-symbolic architecture is trained end-to-end to produce a succinct and effective discrete state transition model from images alone. \\
    \cite{rayyan-242083994}\\\small\href{https://github.com/YanivAspis/Embed2Sym}{Codebase} & \href{https://scholar.archive.org/work/r3ff2msdz5gbdgatkevgg3dmem/access/wayback/https://proceedings.kr.org/2022/44/kr2022-0044-aspis-et-al.pdf}{Embed2sym-scalable neuro-symbolic reasoning via clustered embeddings} & The paper presents Embed2Sym, a scalable framework that combines neural perception and symbolic reasoning through clustered embeddings, enabling fast training, interpretability, and generalization in tasks that exceed the scalability of prior neuro-symbolic systems. Embed2Sym achieves state-of-the-art results and significantly reduces training time on complex reasoning tasks involving visual and symbolic inputs. \\
    \cite{rayyan-242084020}\\\small\href{https://github.com/logictensornetworks/logictensornetworks}{Codebase} & \href{https://doi.org/10.1016/j.artint.2021.103649}{Logic Tensor Networks} & This paper presents Logic Tensor Networks (LTN), a neurosymbolic framework that supports querying, learning and reasoning with both rich data and abstract knowledge about the world. LTN introduces a fully differentiable logical language, called Real Logic, whereby the elements of a first-order logic signature are grounded onto data using neural computational graphs and first-order fuzzy logic semantics \\
    \cite{rayyan-242084029}\\\small\href{https://github.com/microsoft/codeplan}{Codebase} & \href{https://doi.org/10.1145/3643757}{CodePlan: Repository-Level Coding using LLMs and Planning} & The paper introduces CodePlan, a framework that treats large-scale repository-level code edits (e.g., API migrations or temporal edits across many files) as a planning problem: it uses an LLM guided by static dependency and impact analysis to generate a chain of edits until the repository satisfies a correctness oracle. They evaluate on C\# and Python codebases and show that it outperforms no-planning baselines in build success rate and matching ground truth edits. \\
    \cite{rayyan-242084041}\\\small\href{https://github.com/pietrobarbiero/pytorch_explain}{Codebase} & \href{http://proceedings.mlr.press/v202/barbiero23a.html}{Interpretable neural-symbolic concept reasoning} & The paper proposes the Deep Concept Reasoner (DCR), an interpretable concept-based model that uses neural networks to generate fuzzy logic rules from concept embeddings, and then executes those rules on concept truth degrees to make semantically meaningful and differentiable predictions. \\
    \cite{rayyan-242084050}\\\small\href{https://github.com/celestebarnaby/ImageEye}{Codebase} & \href{https://arxiv.org/pdf/2304.03253}{ImageEye: Batch Image Processing using Program Synthesis} & Batch editing of images such as cropping out a desired object/person in a batch of 100+ images has not been an easy task. The paper describes a neuro symbolic approach to generate programs from user demonstrations that perform such tasks. They show the program can automate 96\% of these tasks. \\
    \cite{rayyan-242084132}\\\small\href{https://github.com/SymposiumOrganization/NeuralSymbolicRegressionThatScales}{Codebase} & \href{https://proceedings.mlr.press/v139/biggio21a.html}{Neural symbolic regression that scales} & The paper presents NeSymReS, a neural symbolic regression model that leverages large-scale pre-training of Transformers on procedurally generated equations and data, enabling scalable, efficient, and robust discovery of symbolic equations from input-output data pairs. The approach outperforms traditional and neural symbolic regression methods across diverse evaluation benchmarks. \\
    \cite{rayyan-242084181}\\\small\href{https://github.com/Jaraxxus-Me/LogiCity}{Codebase} & \href{https://proceedings.neurips.cc/paper_files/paper/2024/file/8196be81e68289d7a9ece21ed7f5750a-Paper-Datasets_and_Benchmarks_Track.pdf}{LogiCity: Advancing Neuro-Symbolic AI with Abstract Urban Simulation} & LogiCity models diverse urban elements using semantic and spatial concepts, such as IsAmbulance(X) and IsClose(X, Y). These concepts are used to define FOL rules that govern the behavior of various agents. Since the concepts and rules are abstractions, they can be universally applied to cities with any agent compositions, facilitating the instantiation of diverse scenarios \\
    \cite{rayyan-242084246}\\\small\href{https://github.com/tommasocarraro/NESYKnowledgeTransfer}{Codebase} & \href{https://link.springer.com/chapter/10.1007/978-3-031-56063-7_15}{Mitigating data sparsity via neuro-symbolic knowledge transfer} & Paper uses Logic Tensor Networks (LTN) to mitigate data sparsity issues in recommender systems. It combines Matrix Factorization (neural) models with First-Order Logic axioms (symbolic). \\
    \cite{rayyan-242084271}\\\small\href{https://github.com/chanind/amr-social-chemistry-reasoner}{Codebase} & \href{https://arxiv.org/pdf/2303.08264}{Neuro-symbolic commonsense social reasoning} & present a novel system for taking social rules of thumb (ROTs) in natural language from the Social Chemistry 101 dataset and converting them to first-order logic where reasoning is performed using a neuro-symbolic theorem prover. \\
    \cite{rayyan-242084274}\\\small\href{https://github.com/linqs/dickens-icml24}{Codebase} & \href{https://arxiv.org/abs/2401.09651}{Convex and Bilevel Optimization for Neuro-Symbolic Inference and Learning} & The paper presents a bilevel, gradient-based optimization framework for neural-symbolic (NeSy) learning, introducing a dual block coordinate descent algorithm and smooth formulation for efficient and scalable parameter learning, empirically validated on multiple datasets. \\
    \cite{rayyan-242084302}\\\small\href{https://github.com/Intelligent-CAT-Lab/FlakyDoctor}{Codebase} & \href{https://doi.org/10.1145/3650212.3680369}{Neurosymbolic Repair of Test Flakiness} & Introduces FlakyDoctor, a neuro-symbolic repair system that couples LLM-based patch synthesis with static analysis, compilation “stitching,” and targeted validation to repair order-dependent (OD) and implementation-dependent (ID) flaky tests. Evaluated on 873 flaky tests from 243 projects, achieving \textasciitilde{}57\% OD and 59\% ID repair success and producing 79 previously un-fixed patches (19 merged PRs) \\
    \cite{rayyan-242084309}\\\small\href{https://github.com/czyssrs/ConvFinQA?tab=readme-ov-file}{Codebase} & \href{https://arxiv.org/pdf/2210.03849}{ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering} & Introduces a new dataset to study chain of numerical reasoning in question-answering. \\
    \cite{rayyan-242084344}\\\small\href{https://github.com/claireaoi/hierarchical-rule-induction}{Codebase} & \href{https://arxiv.org/abs/2112.13418}{Neuro-Symbolic Hierarchical Rule Induction} & This paper proposes an efficient and interpretable neuro-symbolic model called Hierarchical Rule Induction (HRI) to solve Inductive Logic Programming (ILP) problems. This model is built from a set of meta-rules organized in a hierarchical structure, and it invents first-order rules by learning embeddings to match facts and predicates. \\
    \cite{rayyan-242084358}\\\small\href{https://github.com/lemonsis/MDD-5k}{Codebase} & \href{https://doi.org/10.1609/aaai.v39i24.34763}{MDD-5k: A New Diagnostic Conversation Dataset for Mental Disorders Synthesized via Neuro-Symbolic LLM Agents} & Introduces a new framework to generate a dataset of diagnostic conversations between a patient and a doctor. It also provides a dataset with 5k high-quality long conversations with diagnosis results and treatment options. \\
    \cite{rayyan-242084360}\\\small\href{https://github.com/linqs/neupsl-ijcai23}{Codebase} & \href{https://dl.acm.org/doi/10.24963/ijcai.2023/461}{NeuPSL: Neural Probabilistic Soft Logic} & Introduce Neural Probabilistic Soft Logic (NeuPSL), a novel neurosymbolic (NeSy) framework that unites state-of-the-art symbolic reasoning with the low-level perception of deep neural networks. To model the boundary between neural and symbolic representations, they propose a family of energy-based models, NeSy Energy-Based Models, and show that they are general enough to include NeuPSL and many other NeSy approaches. \\
    \cite{rayyan-242084397}\\\small\href{https://github.com/itl-ed/llm-dp}{Codebase} & \href{https://arxiv.org/abs/2308.06391}{Dynamic Planning with a LLM} & presents LLM Dynamic Planner (LLM-DP): a neuro-symbolic framework where an LLM works hand-in-hand with a traditional planner to solve an embodied task. Given action-descriptions, LLM-DP solves Alfworld faster and more efficiently than a naive LLM ReAct baseline. \\
    \cite{rayyan-242084398}\\\small\href{https://github.com/vehicle-lang/vehicle}{Codebase} & \href{https://arxiv.org/abs/2401.06379}{Vehicle: Bridging the embedding gap in the verification of neuro-symbolic programs} & This paper presents Vehicle, a tool designed to bridge the "embedding gap" in the verification of neuro-symbolic programs—programs combining neural (machine learning) and symbolic (traditional code) components. Vehicle provides a unified, dependently-typed language and compiler that enables formal specifications for neural components to be integrated across different verification and training backends, demonstrated by verifying a neural network controller for an autonomous car. \\
    \cite{rayyan-242084419}\\\small\href{https://github.com/danyangl6/nn-tli}{Codebase} & \href{https://ieeexplore-ieee-org.proxy-um.researchport.umd.edu/stamp/stamp.jsp?tp=&arnumber=10156357}{Learning Signal Temporal Logic through Neural Network for Interpretable Classification} & design a novel time function and sparse softmax function to improve the soundness and precision of the neural-STL framework. As a result, we can efficiently learn a compact STL formula for the classification of time-series data through off-theshelf gradient-based tools. \\
    \cite{rayyan-242084473}\\\small\href{https://github.com/laurendelong21/MARS?tab=readme-ov-file#replicates}{Codebase} & \href{https://doi.org/10.48550/arXiv.2410.05289}{Mars: A neurosymbolic approach for interpretable drug discovery} & Mechanism of Action Retrieval System (MARS) is a neurosymbolic approach for drug discovery that uses logical rules with learned rule weights. The model has better interpretability through dynamic weight learning from logical rules. \\
    \cite{rayyan-242084497}\\\small\href{https://github.com/vdhanraj/Neurosymbolic-LLM}{Codebase} & \href{https://aclanthology.org/2025.emnlp-main.1556.pdf}{Improving Rule-based Reasoning in LLMs via Neurosymbolic Representations} & introduces a novel neurosymbolic method that improves LLM reasoning by encoding hidden states into neurosymbolic vectors, enabling problem-solving within a neurosymbolic vector space \\
    \cite{rayyan-242084532}\\\small\href{https://github.com/dong-river/CoRRPUS}{Codebase} & \href{https://doi.org/10.18653/v1/2023.findings-acl.832}{CORRPUS: Code-based Structured Prompting for Neurosymbolic Story Understanding} & CoRRPUS shows the usefuleness of code-based symbolic representations for enabling LLMs to perofrm better on story reasoning tasks. \\
    \cite{rayyan-242084533}\\\small\href{https://github.com/google/neural-logic-machines}{Codebase} & \href{https://arxiv.org/pdf/1904.11694}{Neural Logic Machines} & propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning. NLMs exploit the power of both neural networks—as function approximators, and logic programming—as a symbolic processor for objects with properties, relations, logic connectives, and quantifiers. \\
    \cite{rayyan-242084581}\\\small\href{https://github.com/pudumagico/NSGRAPH}{Codebase} & \href{https://ceur-ws.org/Vol-3432/paper11.pdf}{A Modular Neurosymbolic Approach for Visual Graph Question Answering} & This paper presents a modular neuro-symbolic architecture that processes images of graph-structured data (rather than symbolic graphs) by first using optical graph recognition and OCR to extract nodes/edges and labels, then parsing the question and using answer-set programming (ASP) for logical reasoning. This paper also introduces a new VGQA task and dataset (in 3 sets: tiny, small, medium) and establishes a baseline of 73\% accuracy. \\
    \cite{rayyan-242084587}\\\small\href{https://github.com/EleMisi/VAEL}{Codebase} & \href{https://dl.acm.org/doi/10.5555/3600270.3600607}{VAEL: Bridging Variational Autoencoders and Probabilistic Logic Programming} & This paper presents VAEL, a neuro-symbolic generative model integrating variational autoencoders (VAE) with the reasoning capabilities of probabilistic logic (L) programming. Besides standard latent subsymbolic variables, their model exploits a probabilistic logic program to define a further structured representation, which is used for logical reasoning. \\
    \cite{rayyan-242084594}\\\small\href{https://github.com/samuelebortolotti/bears}{Codebase} & \href{https://dl.acm.org/doi/10.5555/3702676.3702790}{BEARS Make Neuro-Symbolic Models Aware of their Reasoning Shortcuts} & They propose to ensure NeSy models are aware of the semantic ambiguity of the concepts they learn, thus enabling their users to identify and distrust low-quality concepts. Starting from three simple desiderata, they derive bears (BE Aware of Reasoning Shortcuts), an ensembling technique that calibrates the model's concept-level confidence without compromising prediction accuracy, thus encouraging NeSy architectures to be uncertain about concepts affected by RSs. \\
    \cite{rayyan-242084600}\\\small\href{https://github.com/HEmile/neurosymbolic-diffusion}{Codebase} & \href{https://arxiv.org/pdf/2505.13138}{Neurosymbolic Diffusion Models} & To overcome the limitations of the independence assumption, this paper introduces neurosymbolic diffusion models (NESYDMS), a new class of NeSy predictors that use discrete diffusion to model dependencies between symbols. their approach reuses the independence assumption from NeSy predictors at each step of the diffusion process, enabling scalable learning while capturing symbol dependencies and uncertainty quantification. \\
    \cite{rayyan-242084603}\\\small\href{https://github.com/HEmile/a-nesi.git}{Codebase} & \href{https://arxiv.org/abs/2212.12393}{A-NeSI: A Scalable Approximate Method for Probabilistic Neurosymbolic Inference} & Introduce Approximate Neurosymbolic Inference (A-NESI): a new framework for PNL that uses neural networks for scalable approximate inference. A-NESI 1) performs approximate inference in polynomial time without changing the semantics of probabilistic logics; 2) is trained using data generated by the background knowledge; 3) can generate symbolic explanations of predictions; and 4) can guarantee the satisfaction of logical constraints at test time, which is vital in safety-critical applications. \\
    \cite{rayyan-242084606}\\\small\href{https://github.com/markendo/HumanMotionQA/tree/master/NSPose}{Codebase} & \href{https://arxiv.org/abs/2305.08953}{Motion Question Answering via Modular Motion Programs} & Proposes a neuro symbolic framework to reason about motion sequences in human actions via symbolic reasoning and modular design. They say that it grounds motion through learning motion concepts, attribute neural operators and temporal relations. The task they chose is human motion QA for evaluate their NSPose method. \\
    \cite{rayyan-242084615}\\\small\href{https://arxiv.org/pdf/2504.19354}{Codebase} & \href{https://arxiv.org/abs/2504.19354}{Neurosymbolic Association Rule Mining from Tabular Data} & Aerial+ trains an autoencoder on tabular data, then extracts association rules by feeding in test vectors with features set to 1. If the autoencoder reconstructs other features with high probability, it creates a rule. Generates way fewer rules than FP-Growth (2-10x less) with better coverage and runs faster on big datasets, plus makes rule-based classifiers like CORELS train faster without losing accuracy. \\
    \cite{rayyan-242084618}\\\small\href{https://github.com/RichardEvans/apperception}{Codebase} & \href{https://www.sciencedirect.com/science/article/pii/S0004370221000722?via%3Dihub}{Making sense of raw input} & The central contribution of this paper is a neuro-symbolic framework for distilling interpretable theories out of streams of raw, unprocessed sensory experience. \\
    \cite{rayyan-242084652}\\\small\href{https://github.com/rfeinman/GNS-Modeling}{Codebase} & \href{https://arxiv.org/abs/2006.14448}{Learning task-general representations with generative neuro-symbolic modeling} & develop a generative neuro-symbolic (GNS) model of handwritten character concepts that uses the control flow of a probabilistic program, coupled with symbolic stroke primitives and a symbolic image renderer, to represent the causal and compositional processes by which characters are formed. \\
    \cite{rayyan-242084661}\\\small\href{https://github.com/feng-yufei/NS-NLI?tab=readme-ov-file}{Codebase} & \href{10.1162/tacl_a_00458}{Neuro-symbolic natural logic with introspective revision for natural language inference} & Presents a NeSy framework to integrate natural logic with NNs to do natural language inference. They use RL to help guide this natural logic. Overall this improves the intuitive inference understandability of humans. \\
    \cite{rayyan-242084782}\\\small\href{https://github.com/NanxuGong/feature-selection-via-autoregreesive-generation}{Codebase} & \href{https://arxiv.org/abs/2404.17157}{Neuro-Symbolic Embedding for Short and Effective Feature Selection via Autoregressive Generation} & Proposes a neuro-symbolic framework that models feature selection as an autoregressive token generation problem, learning embeddings for short and effective feature subsets with superior predictive performance compared to classical baselines. \\
    \cite{rayyan-242084825}\\\small\href{https://github.com/sbhakim/ansr-dt}{Codebase} & \href{https://arxiv.org/abs/2501.08561}{ANSR-DT: An Adaptive Neuro-Symbolic Learning and Reasoning Framework for Digital Twins} & ANSR-DT presents an adaptive neuro-symbolic framework integrating deep learning (CNN-LSTM) and symbolic reasoning (Prolog) for digital twins. The system generates interpretable insights, learns rules from data, and provides knowledge graph visualizations to support decision-making in industrial scenarios. \\
    \cite{rayyan-242084891}\\\small\href{https://github.com/henrijsprincis/Xander}{Codebase} & \href{https://arxiv.org/abs/2408.13888}{Enhancing SQL Query Generation with Neurosymbolic Reasoning} & Find a way to use language models to generate SQL queries. They introduce a new tool called Xander which helps \\
    \cite{rayyan-242084922}\\\small\href{https://github.com/hftsoi/SymbolNet}{Codebase} & \href{https://arxiv.org/abs/2401.09949}{SymbolNet: Neural Symbolic Regression with Adaptive Dynamic Pruning} & SymbolNet combines neural networks with symbolic regression through adaptive pruning to extract interpretable mathematical expressions while achieving high model compression. The method uses a custom training loop with threshold-based pruning to identify and eliminate less important network connections, enabling recovery of human-readable symbolic formulas from trained models without sacrificing predictive accuracy. \\
    \cite{rayyan-242084944}\\\small\href{https://github.com/IntelLabs/multimodal_cognitive_ai/tree/main/NeuroComparatives}{Codebase} & \href{https://arxiv.org/abs/2305.04978}{Neurocomparatives: Neuro-symbolic distillation of comparative knowledge} & This paper presents NeuroComparatives, a dataset of 8.8M comparative commonsense statements (e.g., "cats are typically smaller than dogs") generated using constrained beam search with GPT-2 and filtered using a discriminator trained on human annotations. The dataset aims to provide high-validity comparative knowledge for commonsense reasoning tasks. \\
    \cite{rayyan-242084952}\\\small\href{https://github.com/ant-research/StructuredLM_RTDT}{Codebase} & \href{https://arxiv.org/pdf/2303.02860}{A multi-grained self-interpretable symbolic-neural model for single/multi-labeled text classification} & propose a Symbolic-Neural model that can learn to explicitly predict class labels of text spans from a constituency tree without requiring any access to spanlevel gold labels \\
    \cite{rayyan-242084996}\\\small\href{https://github.com/azreasoners/CRCG}{Codebase} & \href{https://arxiv.org/pdf/2506.10753}{Think before You Simulate: Symbolic Reasoning to Orchestrate Neural Computation for Counterfactual Question Answering} & The paper discusses a novel approach to do causal and temporal reasoning in video particularly for counterfactual reasoning. They make use of a causal graph and ASP to coordinate between perception and simulation modules. \\
    \cite{rayyan-242085029}\\\small\href{https://github.com/ML-KULeuven/klay?tab=readme-ov-file}{Codebase} & \href{https://arxiv.org/abs/2410.11415}{KLay: Accelerating Arithmetic Circuits for Neurosymbolic AI} & The paper presents KLay, a scalable data structure and layerization algorithm that accelerates arithmetic circuit computation in neurosymbolic AI systems, enabling efficient forward and backward passes. It demonstrates near-linear runtime scaling across five orders of magnitude, significantly outperforming baseline methods on logic-based inference tasks. \\
    \cite{rayyan-242085060}\\\small\href{https://github.com/Jie0618/PhysicsRegression}{Codebase} & \href{https://arxiv.org/abs/2503.07994}{A Neural Symbolic Model for Space Physics} & PhyE2E presents an end-to-end transformer-based framework for physics-informed symbolic regression that incorporates dimensional analysis as an inductive bias through oracle-guided divide-and-conquer strategies combined with MCTS refinement. The method achieves state-of-the-art performance on the Feynman equations benchmark, improving symbolic accuracy by 10-29\% over existing methods like PySR, uDSR, TPSR, and LaSR. \\
    \cite{rayyan-242085081}\\\small\href{https://github.com/lab-v2/PyEDCR}{Codebase} & \href{https://arxiv.org/pdf/2407.15192}{Error Detection and Constraint Recovery in Hierarchical Multi-Label Classification without Prior Knowledge} & his paper presents an approach that uses Error Detection Rules (EDR) to learn the failure modes of machine learning models, relaxing the common assumption that hierarchical constraints must exist beforehand. These learned rules are effective at detecting classifier errors and can be leveraged as constraints for Hierarchical Multi-label Classification (HMC), allowing for the recovery of explainable constraints even when they are not provided initially. \\
    \cite{rayyan-242085093}\\\small\href{https://github.com/Mayer123/HyKAS-CSKG}{Codebase} & \href{https://ojs.aaai.org/index.php/AAAI/article/view/17593/17400}{Knowledge-driven Data Construction for Zero-shot Evaluation in Commonsense Question Answering} & vary the set of language models, training regimes, knowledge sources, and data generation strategies, and measure their impact across tasks. Extending on prior work, we devise and compare four constrained distractor-sampling strategies \\
    \cite{rayyan-242085117}\\\small\href{https://github.com/garrettkatz/poppy-muffin}{Codebase} & \href{https://doi.org/10.3389/fnbot.2021.744031}{Tunable Neural Encoding of a Symbolic Robotic Manipulation Algorithm} & present a neurocomputational controller for robotic manipulation based on the recently developed “neural virtual machine” (NVM). Authors program the NVM with a symbolic algorithm that solves blocks-world restacking problems, and execute it in a robotic simulation environment \\
    \cite{rayyan-242085129}\\\small\href{https://github.com/alex-calderwood/there-and-back}{Codebase} & \href{https://ojs.aaai.org/index.php/AIIDE/article/view/27502}{There and back again: extracting formal domains for controllable neurosymbolic story authoring} & Demonstrate that languagemodels can be used to author narrative planning domainsfrom natural language stories with minimal human intervention. Second, authors explore the reverse, demonstrating that we can use logical story domains and plans to produce storiesthat respect the narrative commitments of the planne \\
    \cite{rayyan-242085166}\\\small\href{https://github.com/HRI-EU/ProMis}{Codebase} & \href{10.1109/TITS.2025.3609835}{Probabilistic Mission Design in Neuro-Symbolic Systems} & Authors propose Probabilistic Mission Design (ProMis), a novel neuro-symbolic approach to navigating UAS within legal frameworks. ProMis is an interpretable and adaptable system architecture that links uncertain geospatial data and noisy perception with declarative, Hybrid Probabilistic Logic Programs (HPLP) to reason over the agent’s state space and its legality \\
    \cite{rayyan-242085167}\\\small\href{https://github.com/HRI-EU/ProMis/tree/cofi}{Codebase} & \href{10.48550/arXiv.2412.18347}{The Constitutional Filter} & introduces an approach for Bayesian estimation of agents expected to comply with a human-interpretable neuro-symbolic model we call its Constitution. Hence, autors present the Constitutional Filter (CoFi), leading to improved tracking of agents by leveraging expert knowledge, incorporating deep learning architectures, and accounting for environmental uncertainties \\
    \cite{rayyan-242085185}\\\small\href{https://github.com/NeuralMAS/venmas}{Codebase} & \href{https://ceur-ws.org/Vol-3819/short1.pdf}{Formal verification of parameterised neural-symbolic multi-agent systems} & This paper presents VENMAS, a formal verification toolkit for multi-agent systems with neural components, enabling bounded temporal logic verification of parameterized neural-symbolic agents. The work extends existing verification methods to handle systems with an arbitrary number of homogeneous agents using symbolic abstraction techniques. \\
    \cite{rayyan-242085203}\\\small\href{https://github.com/bio-ontology-research-group/machine-learning-with-ontologies}{Codebase} & \href{https://doi.org/10.1093/bib/bbaa199}{Semantic similarity and machine learning with ontologies} & provide an overview over the methods that use ontologies to compute similarity and incorporate them in machine learning methods; in particular, the authors outline how semantic similarity measures and ontology embeddings can exploit the background knowledge in ontologies and how ontologies can provide constraints that improve machine learning models. \\
    \cite{rayyan-242085204}\\\small\href{https://github.com/bio-ontology-research-group/deepgo2}{Codebase} & \href{https://www.nature.com/articles/s42256-024-00795-w}{Protein function prediction as approximate semantic entailment} & The Gene Ontology (GO) is a formal, axiomatic theory with over 100,000 axioms that describe the molecular functions, biological processes and cellular locations of proteins in three subontologies. Developed DeepGO-SE, a method that predicts GO functions from protein sequences using a pretrained large language model. DeepGO-SE generates multiple approximate models of GO, and a neural network predicts the truth values of statements about protein functions in these approximate models. \\
    \cite{rayyan-242085252}\\\small\href{https://github.com/THU-KEG/DiaKoP}{Codebase} & \href{https://dl.acm.org/doi/pdf/10.1145/3627673.3679229}{Diakop: Dialogue-based knowledge-oriented programming for neural-symbolic knowledge base question answering} & present Dialogue-based Knowledge-oriented Programming system (DiaKoP), a system with a chat interface designed for multi-turn knowledge base question answering (KBQA). DiaKoP enables users to decompose complex questions into multiple simpler follow-up questions and interact with the system to obtain answers. \\
    \cite{rayyan-242085266}\\\small\href{https://github.com/liqing-ustc/NGS}{Codebase} & \href{https://proceedings.mlr.press/v119/li20f.html}{Closed loop neural-symbolic learning via integrating neural perception, grammar parsing, and symbolic reasoning} & This paper proposes a Neural-Grammar-Symbolic (NGS) model that improves upon inefficient reinforcement learning approaches by using a grammar model to bridge neural perception and symbolic reasoning, along with a novel back-search algorithm to learn from incorrect predictions. The experiments, conducted on handwritten formula recognition and visual question answering tasks, demonstrate that this method significantly outperforms reinforcement learning models in terms of performance, convergence speed, and data efficiency. \\
    \cite{rayyan-242085281}\\\small\href{https://github.com/nju-websoft/AdaLoGN}{Codebase} & \href{https://aclanthology.org/2022.acl-long.494.pdf}{AdaLoGN: Adaptive Logic Graph Network for Reasoning-Based Machine Reading Comprehension} & present a neural-symbolic approach which, to predict an answer, passes messages over a graph representing logical relations between text units. \\
    \cite{rayyan-242085286}\\\small\href{https://github.com/scallop-lang/scallop}{Codebase} & \href{https://dl.acm.org/doi/10.1145/3591280}{Scallop: A language for neurosymbolic programming} & present Scallop, a language which combines the benefits of deep learning and logical reasoning. Scallop enables users to write a wide range of neurosymbolic applications and train them in a data- and compute-efficient manner. It achieves these goals through three key features: 1) a flexible symbolic representation that is based on the relational data model; 2) a declarative logic programming language that is based on Datalog and supports recursion, aggregation, and negation; and 3) a framework for automatic and efficient differentiable reasoning that is based on the theory of provenance semirings. \\
    \cite{rayyan-242085297}\\\small\href{https://github.com/scallop-lang/scallop}{Codebase} & \href{https://doi.org/10.1609/aaai.v38i9.28934}{Relational Programming with Foundation Models} & implement VIEIRA by extending the SCALLOP compiler with a foreign interface that supports foundation models as plugins. We implement plugins for 12 foundation models including GPT, CLIP, and SAM. We evaluate VIEIRA on 9 challenging tasks that span language, vision, and structured and vector databases. Our evaluation shows that programs in VIEIRA are concise, can incorporate modern foundation models, and have comparable or better accuracy than competitive baselines. \\
    \cite{rayyan-242085341}\\\small\href{https://github.com/loganrjmurphy/LeanEuclid}{Codebase} & \href{https://dl.acm.org/doi/10.5555/3692070.3693567}{Autoformalizing Euclidean Geometry} & use theorem provers to fill in such diagrammatic information automatically, so that the LLM only needs to autoformalize the explicit textual steps, making it easier for the model. \\
    \cite{rayyan-242085387}\\\small\href{https://github.com/nmanginas/nesya}{Codebase} & \href{https://www.ijcai.org/proceedings/2025/0662.pdf}{NeSyA: Neurosymbolic Automata} & how that symbolic automata can be integrated with neural-based perception, under probabilistic semantics towards an end-to-end differentiable model. Their proposed hybrid model, termed NESYA (Neuro Symbolic Automata) is shown to either scale or perform more accurately than previous NeSy systems in a synthetic benchmark and to provide benefits in terms of generalization compared to purely neural systems in a realworld event recognition task \\
    \cite{rayyan-242085427}\\\small\href{https://github.com/mmejri3/LARS-VSA}{Codebase} & \href{https://www.researchgate.net/publication/380820165_LARS-VSA_A_Vector_Symbolic_Architecture_For_Learning_with_Abstract_Rules}{LARS-VSA: A Vector Symbolic Architecture For Learning with Abstract Rules} & The paper presents LARS-VSA, a neuro-symbolic framework leveraging hyperdimensional computing for abstract rule learning with compositional generalization. It introduces a high-dimensional attention mechanism and demonstrates superior generalization, efficiency, and accuracy over contemporary neural and neuro-symbolic baselines on multiple relational reasoning and math tasks. \\
    \cite{rayyan-242085538}\\\small\href{https://github.com/benlipkin/linc}{Codebase} & \href{https://aclanthology.org/2023.emnlp-main.313.pdf}{(sic) LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers} & investigate the validity of instead reformulating such tasks as modular neurosymbolic programming, which we call LINC: Logical Inference via Neurosymbolic Computation. In LINC, the LLM acts as a semantic parser, translating premises and conclusions from natural language to expressions in first-order logic. These expressions are then offloaded to an external theorem prover, which symbolically performs deductive inference. \\
    \cite{rayyan-242085624}\\\small\href{https://github.com/clairepost/AMRtoUMR}{Codebase} & \href{https://aclanthology.org/2024.dmr-1.15/}{Accelerating UMR adoption: Neuro-symbolic conversion from AMR-to-UMR with low supervision} & The paper proposes a neuro-symbolic method for converting AMR (Abstract Meaning Representation) roles to UMR (Uniform Meaning Representation) roles by integrating animacy parsing and logic rules with a neural network. The approach addresses non-deterministic role mappings with minimal human supervision, achieving 75.81\% accuracy compared to a 62.35\% baseline neural network. \\
    \cite{rayyan-242085638}\\\small\href{https://github.com/HLR/SpaRTUNQChain}{Codebase} & \href{https://arxiv.org/abs/2406.13828}{Neuro-symbolic Training for Reasoning over Spatial Language} & This paper presents SpaRTUNQChain, a neuro-symbolic framework that combines BERT embeddings with spatial logic constraints for question-answering over spatial language. The system uses the DomiKnowS framework to enforce compositional reasoning rules during training, improving accuracy on spatial reasoning benchmarks. \\
    \cite{rayyan-242085659}\\\small\href{https://github.com/neuro-symbolic-ai/peirce}{Codebase} & \href{https://aclanthology.org/2025.acl-demo.2.pdf}{Peirce: Unifying material and formal reasoning via llm-driven neuro-symbolic refinement} & introduce PEIRCE, a neuro-symbolic framework designed to unify material and formal inference through an iterative conjecture–criticism process. \\
    \cite{rayyan-242085726}\\\small\href{https://github.com/MarcRoigVilamala/DeepProbCEP}{Codebase} & \href{https://doi.org/10.1016/j.eswa.2022.119376}{DeepProbCEP: A neuro-symbolic approach for complex event processing in adversarial settings} & Introduces DeepProbCEP, a differentiable ProbLog + CNN hybrid that trains end‑to‑end from complex‑event labels. \\
    \cite{rayyan-242085892}\\\small\href{https://github.com/ml-research/SLASH}{Codebase} & \href{https://jair.org/index.php/jair/article/view/15027}{Scalable Neural-Probabilistic Answer Set Programming} & This paper introduces Answer Set Networks (ASNs), a novel neural-symbolic solver based on Graph Neural Networks (GNNs) designed to overcome the high computational costs and CPU-bound nature of traditional Answer Set Programming (ASP) solvers. By translating ASP problems into a graph format that leverages GPU parallelization, ASNs outperform existing systems and enable new applications like fine-tuning Large Language Models (LLMs) with logic and solving large-scale drone navigation tasks. \\
    \cite{rayyan-242085977}\\\small\href{https://github.com/tammet/nlpsolver}{Codebase} & \href{https://link.springer.com/chapter/10.1007/978-3-031-38499-8_29}{An Experimental Pipeline for Automated Reasoning in Natural Language (Short Paper)} & The paper presents NLPSolver, an experimental end-to-end pipeline that parses English text with a neural UD parser, converts it into extended first-order logic with defaults and confidences, performs reasoning with a high-performance default logic engine, and converts proofs back into natural language answers and explanations. The authors evaluate the system on toy NLI/QA examples, a subset of HANS, and AllenAI ProofWriter demos, showing strong performance there and highlighting the pipeline as a basis for future hybrid neuro-symbolic systems. \\
    \cite{rayyan-242086000}\\\small\href{https://github.com/ML-KULeuven/deepstochlog}{Codebase} & \href{https://arxiv.org/pdf/2106.12574}{DeepStochLog: Neural Stochastic Logic Programming} & This paper introduces DeepStochLog, a novel neural-symbolic framework that integrates neural networks into stochastic definite clause grammars (SDCGs) to define a probability distribution over possible derivations. The experimental evaluation shows this approach scales significantly better than methods based on neural probabilistic logic programs and achieves state-of-the-art results on several challenging neural-symbolic tasks. \\
    \cite{rayyan-242086029}\\\small\href{https://github.com/google-deepmind/alphageometry}{Codebase} & \href{https://www.nature.com/articles/s41586-023-06747-5}{Solving olympiad geometry without human demonstrations} & AlphaGeometry is a neuro-symbolic system that uses a neural language model, trained from scratch on our large-scale synthetic data, to guide a symbolic deduction engine through infinite branching points in challenging problems. \\
    \cite{rayyan-242086133}\\\small\href{https://github.com/SoftWiser-group/LaM4Inv}{Codebase} & \href{https://dl-acm-org.proxy-um.researchport.umd.edu/doi/pdf/10.1145/3691620.3695014}{LLM Meets Bounded Model Checking: Neuro-symbolic Loop Invariant Inference} & proposes LaM4Inv, a neuro-symbolic framework that combines large language models (Llama-3-8B, GPT-3.5, GPT-4, GPT-4-Turbo) with bounded model checking (ESBMC + SMT solvers) to automatically infer loop invariants for C programs. It evaluates LaM4Inv on an expanded benchmark of 316 loop-invariant problems, showing large gains in the number of verified loops compared to classical invariant generators and recent LLM-based baselines. \\
    \cite{rayyan-242086137}\\\small\href{https://ieeexplore.ieee.org/abstract/document/10680497}{Codebase} & \href{https://ieeexplore.ieee.org/abstract/document/10680497/}{MILE: Memory-Interactive Learning Engine for Neuro-Symbolic Solutions to Mathematical Problems} & MILE is a neuro-symbolic math word problem solver that predicts formulas using a memory-based decoder instead of tree-structured decoding, letting it handle more complex computation graphs. It beats existing methods on Math23K by learning formulas as rules. \\
    \cite{rayyan-242086150}\\\small\href{https://github.com/kangxin/NCAI}{Codebase} & \href{https://aclanthology.org/2025.neusymbridge-1.8.pdf}{Neuro-Conceptual Artificial Intelligence: Integrating OPM with Deep Learning to Enhance Question Answering Quality} & introduceNeuro-ConceptualArtificial Intelligence(NCAI),aspecializationof the neuro-symbolicAI approach that integrates conceptual modeling usingObject-Process Methodology (OPM) ISO19450:2024with deeplearningtoenhancequestion-answering (QA)quality \\
    \cite{rayyan-242086183}\\\small\href{https://github.com/azreasoners/recurrent_transformer}{Codebase} & \href{https://openreview.net/forum?id=udNhDCr2KQe}{Learning to Solve Constraint Satisfaction Problems with Recurrent Transformer} & Constraint satisfaction problems (CSPs) are about finding values of variables that satisfy the given constraints. We show that Transformer extended with recurrence is a viable approach to learning to solve CSPs in an end-to-end manner, having clear advantages over state-of-the-art methods such as Graph Neural Networks, SATNet, and some neuro-symbolic models. \\
    \cite{rayyan-242086184}\\\small\href{https://github.com/azreasoners/cl-ste}{Codebase} & \href{https://proceedings.mlr.press/v162/yang22h/yang22h.pdf}{Injecting Logical Constraints into Neural Networks via Straight-Through Estimators} & This paper introduces CL-STE, a method to inject discrete logical constraints into neural networks by representing the constraints as a loss function and using a Straight-Through Estimator (STE) to enable gradient-based optimization. By leveraging GPUs and avoiding heavy symbolic computation, this technique scales significantly better than existing neuro-symbolic methods and allows various architectures, like CNNs and GNNs, to learn from constraints with fewer or no labeled data. \\
    \cite{rayyan-242086220}\\\small\href{https://github.com/Lizn-zn/NeqLIPS}{Codebase} & \href{https://arxiv.org/pdf/2502.13834}{Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning} & we introduce a neuro-symbolic tactic generator that synergizes the mathematical intuition learned by LLMs with domain-specific insights encoded by symbolic methods. \\
\end{longtable}
