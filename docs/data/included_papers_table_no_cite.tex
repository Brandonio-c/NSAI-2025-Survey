% This table requires the longtable package.
% Add \usepackage{longtable} to your document preamble.
%
% All special LaTeX characters (including underscores) have been properly escaped.
% This table spans the full page width and includes horizontal lines between entries.
%
\begin{onecolumn}
\begin{longtable}{|p{0.22\textwidth}|p{0.28\textwidth}|p{0.48\textwidth}|}
\hline
\textbf{Citation} & \textbf{Title} & \textbf{Description} \\
\hline
\endfirsthead

\hline
\textbf{Citation} & \textbf{Title} & \textbf{Description} \\
\hline
\endhead

\hline
\endfoot

\hline
\endlastfoot

    \small\raggedright Murali et al. (2019) & Composing Neural Learning and Symbolic Reasoning with an Application to Visual Discrimination & propose a compositional neurosymbolic framework that combines a neural network to detect objects and relationships with a symbolic learner that finds interpretable discriminators. \\
\hline
    \small\raggedright Ahmed et al. (2022) & Semantic probabilistic layers for neuro-symbolic learning & The paper presents Semantic Probabilistic Layers (SPLs), a neural network module that ensures structured-output predictions are always consistent with logical constraints, enabling accurate and tractable neuro-symbolic learning. SPLs modularly combine probabilistic inference and logical reasoning, outperforming previous methods in tasks requiring strict output validity. \\
\hline
    \small\raggedright Ahmed et al. (2022) & Neuro-symbolic entropy regularization & This paper presents neuro-symbolic entropy regularization, a unified framework that combines entropy regularization and neuro-symbolic learning for structured prediction tasks. By constraining entropy minimization to outputs that form valid structures (as defined by logical circuits), the approach yields models that are both more accurate and more likely to produce valid predictions, demonstrated across semi-supervised and fully-supervised experiments. \\
\hline
    \small\raggedright Akl (2024) & NeSy is alive and well: A LLM-driven symbolic approach for better code comment data generation and classification & This paper presents a neuro-symbolic workflow combining semantic rule–based decomposition with an LLM to generate controlled synthetic data for C-code comment classification. Empirical results show that this augmentation improves ML models (Voting Classifier, Random Forest, MLP) performance measured in F1 score. \\
\hline
    \small\raggedright Alam et al. (2024) & A Walsh Hadamard Derived Linear Vector Symbolic Architecture & The paper introduces the Hadamard-derived Linear Binding (HLB), a novel vector symbolic architecture that leverages properties of the Walsh-Hadamard transform for efficient, numerically stable vector binding in neuro-symbolic AI. HLB achieves state-of-the-art performance on both classical VSA benchmarks and selected deep learning tasks, outperforming previous VSA methods in terms of computational complexity, accuracy, and differentiability for modern neural architectures. \\
\hline
    \small\raggedright Ibrahimzada et al. (2024) & AlphaTrans: A Neuro-Symbolic Compositional Approach for Repository-Level Code Translation and Validation & Solves the task of code translation from one programming language to another. It does so using neural symbolic framework that breaks down the source code into fragments and also utilized the test code to ensure code was properly translated. \\
\hline
    \small\raggedright Alon et al. (2022) & Neuro-symbolic language modeling with automaton-augmented retrieval & The paper presents RETOMATON, a neuro-symbolic system that approximates costly nearest-neighbor datastore searches in retrieval-based language models using automaton states and pointer links, enabling substantial speed-ups while maintaining or improving perplexity. \\
\hline
    \small\raggedright Asai and Muise (2020) & Learning neural-symbolic descriptive planning models via cube-space priors: The voyage home (to STRIPS) & neuro-symbolic architecture is trained end-to-end to produce a succinct and effective discrete state transition model from images alone. \\
\hline
    \small\raggedright Badreddine et al. (2022) & Logic Tensor Networks & This paper presents Logic Tensor Networks (LTN), a neurosymbolic framework that supports querying, learning and reasoning with both rich data and abstract knowledge about the world. LTN introduces a fully differentiable logical language, called Real Logic, whereby the elements of a first-order logic signature are grounded onto data using neural computational graphs and first-order fuzzy logic semantics \\
\hline
    \small\raggedright Bairi et al. (2024) & CodePlan: Repository-Level Coding using LLMs and Planning & The paper introduces CodePlan, a framework that treats large-scale repository-level code edits (e.g., API migrations or temporal edits across many files) as a planning problem: it uses an LLM guided by static dependency and impact analysis to generate a chain of edits until the repository satisfies a correctness oracle. They evaluate on C\# and Python codebases and show that it outperforms no-planning baselines in build success rate and matching ground truth edits. \\
\hline
    \small\raggedright Barbiero et al. (2023) & Interpretable neural-symbolic concept reasoning & The paper proposes the Deep Concept Reasoner (DCR), an interpretable concept-based model that uses neural networks to generate fuzzy logic rules from concept embeddings, and then executes those rules on concept truth degrees to make semantically meaningful and differentiable predictions. \\
\hline
    \small\raggedright Barnaby et al. (2023) & ImageEye: Batch Image Processing using Program Synthesis & Batch editing of images such as cropping out a desired object/person in a batch of 100+ images has not been an easy task. The paper describes a neuro symbolic approach to generate programs from user demonstrations that perform such tasks. They show the program can automate 96\% of these tasks. \\
\hline
    \small\raggedright Biggio et al. (2021) & Neural symbolic regression that scales & The paper presents NeSymReS, a neural symbolic regression model that leverages large-scale pre-training of Transformers on procedurally generated equations and data, enabling scalable, efficient, and robust discovery of symbolic equations from input-output data pairs. The approach outperforms traditional and neural symbolic regression methods across diverse evaluation benchmarks. \\
\hline
    \small\raggedright Li et al. (2024) & LogiCity: Advancing Neuro-Symbolic AI with Abstract Urban Simulation & LogiCity models diverse urban elements using semantic and spatial concepts, such as IsAmbulance(X) and IsClose(X, Y). These concepts are used to define FOL rules that govern the behavior of various agents. Since the concepts and rules are abstractions, they can be universally applied to cities with any agent compositions, facilitating the instantiation of diverse scenarios \\
\hline
    \small\raggedright Carraro et al. (2024) & Mitigating data sparsity via neuro-symbolic knowledge transfer & Paper uses Logic Tensor Networks (LTN) to mitigate data sparsity issues in recommender systems. It combines Matrix Factorization (neural) models with First-Order Logic axioms (symbolic). \\
\hline
    \small\raggedright Chanin and Hunter (2023) & Neuro-symbolic commonsense social reasoning & present a novel system for taking social rules of thumb (ROTs) in natural language from the Social Chemistry 101 dataset and converting them to first-order logic where reasoning is performed using a neuro-symbolic theorem prover. \\
\hline
    \small\raggedright Dickens et al. (2024) & Convex and Bilevel Optimization for Neuro-Symbolic Inference and Learning & The paper presents a bilevel, gradient-based optimization framework for neural-symbolic (NeSy) learning, introducing a dual block coordinate descent algorithm and smooth formulation for efficient and scalable parameter learning, empirically validated on multiple datasets. \\
\hline
    \small\raggedright Chen and Jabbarvand (2024) & Neurosymbolic Repair of Test Flakiness & Introduces FlakyDoctor, a neuro-symbolic repair system that couples LLM-based patch synthesis with static analysis, compilation “stitching,” and targeted validation to repair order-dependent (OD) and implementation-dependent (ID) flaky tests. Evaluated on 873 flaky tests from 243 projects, achieving \textasciitilde{}57\% OD and 59\% ID repair success and producing 79 previously un-fixed patches (19 merged PRs) \\
\hline
    \small\raggedright Chen et al. (2022) & ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering & Introduces a new dataset to study chain of numerical reasoning in question-answering. \\
\hline
    \small\raggedright Glanois et al. (2021) & Neuro-Symbolic Hierarchical Rule Induction & This paper proposes an efficient and interpretable neuro-symbolic model called Hierarchical Rule Induction (HRI) to solve Inductive Logic Programming (ILP) problems. This model is built from a set of meta-rules organized in a hierarchical structure, and it invents first-order rules by learning embeddings to match facts and predicates. \\
\hline
    \small\raggedright Yin et al. (2024) & MDD-5k: A New Diagnostic Conversation Dataset for Mental Disorders Synthesized via Neuro-Symbolic LLM Agents & Introduces a new framework to generate a dataset of diagnostic conversations between a patient and a doctor. It also provides a dataset with 5k high-quality long conversations with diagnosis results and treatment options. \\
\hline
    \small\raggedright Pryor et al. (2022) & NeuPSL: Neural Probabilistic Soft Logic & Introduce Neural Probabilistic Soft Logic (NeuPSL), a novel neurosymbolic (NeSy) framework that unites state-of-the-art symbolic reasoning with the low-level perception of deep neural networks. To model the boundary between neural and symbolic representations, they propose a family of energy-based models, NeSy Energy-Based Models, and show that they are general enough to include NeuPSL and many other NeSy approaches. \\
\hline
    \small\raggedright Dagan et al. (2023) & Dynamic Planning with a LLM & presents LLM Dynamic Planner (LLM-DP): a neuro-symbolic framework where an LLM works hand-in-hand with a traditional planner to solve an embodied task. Given action-descriptions, LLM-DP solves Alfworld faster and more efficiently than a naive LLM ReAct baseline. \\
\hline
    \small\raggedright Daggitt et al. (2024) & Vehicle: Bridging the embedding gap in the verification of neuro-symbolic programs & This paper presents Vehicle, a tool designed to bridge the "embedding gap" in the verification of neuro-symbolic programs—programs combining neural (machine learning) and symbolic (traditional code) components. Vehicle provides a unified, dependently-typed language and compiler that enables formal specifications for neural components to be integrated across different verification and training backends, demonstrated by verifying a neural network controller for an autonomous car. \\
\hline
    \small\raggedright Li et al. (2022) & Learning Signal Temporal Logic through Neural Network for Interpretable Classification & design a novel time function and sparse softmax function to improve the soundness and precision of the neural-STL framework. As a result, we can efficiently learn a compact STL formula for the classification of time-series data through off-theshelf gradient-based tools. \\
\hline
    \small\raggedright DeLong et al. (2024) & Mars: A neurosymbolic approach for interpretable drug discovery & Mechanism of Action Retrieval System (MARS) is a neurosymbolic approach for drug discovery that uses logical rules with learned rule weights. The model has better interpretability through dynamic weight learning from logical rules. \\
\hline
    \small\raggedright Dhanraj and Eliasmith (2025) & Improving Rule-based Reasoning in LLMs via Neurosymbolic Representations & introduces a novel neurosymbolic method that improves LLM reasoning by encoding hidden states into neurosymbolic vectors, enabling problem-solving within a neurosymbolic vector space \\
\hline
    \small\raggedright Done et al. (2023) & CORRPUS: Code-based Structured Prompting for Neurosymbolic Story Understanding & CoRRPUS shows the usefuleness of code-based symbolic representations for enabling LLMs to perofrm better on story reasoning tasks. \\
\hline
    \small\raggedright Dong et al. (2019) & Neural Logic Machines & propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning. NLMs exploit the power of both neural networks—as function approximators, and logic programming—as a symbolic processor for objects with properties, relations, logic connectives, and quantifiers. \\
\hline
    \small\raggedright Eiter et al. (2023) & A Modular Neurosymbolic Approach for Visual Graph Question Answering & This paper presents a modular neuro-symbolic architecture that processes images of graph-structured data (rather than symbolic graphs) by first using optical graph recognition and OCR to extract nodes/edges and labels, then parsing the question and using answer-set programming (ASP) for logical reasoning. This paper also introduces a new VGQA task and dataset (in 3 sets: tiny, small, medium) and establishes a baseline of 73\% accuracy. \\
\hline
    \small\raggedright Misino et al. (2022) & VAEL: Bridging Variational Autoencoders and Probabilistic Logic Programming & This paper presents VAEL, a neuro-symbolic generative model integrating variational autoencoders (VAE) with the reasoning capabilities of probabilistic logic (L) programming. Besides standard latent subsymbolic variables, their model exploits a probabilistic logic program to define a further structured representation, which is used for logical reasoning. \\
\hline
    \small\raggedright Marconato et al. (2024) & BEARS Make Neuro-Symbolic Models Aware of their Reasoning Shortcuts & They propose to ensure NeSy models are aware of the semantic ambiguity of the concepts they learn, thus enabling their users to identify and distrust low-quality concepts. Starting from three simple desiderata, they derive bears (BE Aware of Reasoning Shortcuts), an ensembling technique that calibrates the model's concept-level confidence without compromising prediction accuracy, thus encouraging NeSy architectures to be uncertain about concepts affected by RSs. \\
\hline
    \small\raggedright van Krieken et al. (2025) & Neurosymbolic Diffusion Models & To overcome the limitations of the independence assumption, this paper introduces neurosymbolic diffusion models (NESYDMS), a new class of NeSy predictors that use discrete diffusion to model dependencies between symbols. their approach reuses the independence assumption from NeSy predictors at each step of the diffusion process, enabling scalable learning while capturing symbol dependencies and uncertainty quantification. \\
\hline
    \small\raggedright van Krieken et al. (2022) & A-NeSI: A Scalable Approximate Method for Probabilistic Neurosymbolic Inference & Introduce Approximate Neurosymbolic Inference (A-NESI): a new framework for PNL that uses neural networks for scalable approximate inference. A-NESI 1) performs approximate inference in polynomial time without changing the semantics of probabilistic logics; 2) is trained using data generated by the background knowledge; 3) can generate symbolic explanations of predictions; and 4) can guarantee the satisfaction of logical constraints at test time, which is vital in safety-critical applications. \\
\hline
    \small\raggedright Endo et al. (2023) & Motion Question Answering via Modular Motion Programs & Proposes a neuro symbolic framework to reason about motion sequences in human actions via symbolic reasoning and modular design. They say that it grounds motion through learning motion concepts, attribute neural operators and temporal relations. The task they chose is human motion QA for evaluate their NSPose method. \\
\hline
    \small\raggedright Karabulut et al. (2025) & Neurosymbolic Association Rule Mining from Tabular Data & Aerial+ trains an autoencoder on tabular data, then extracts association rules by feeding in test vectors with features set to 1. If the autoencoder reconstructs other features with high probability, it creates a rule. Generates way fewer rules than FP-Growth (2-10x less) with better coverage and runs faster on big datasets, plus makes rule-based classifiers like CORELS train faster without losing accuracy. \\
\hline
    \small\raggedright Evans et al. (2021) & Making sense of raw input & The central contribution of this paper is a neuro-symbolic framework for distilling interpretable theories out of streams of raw, unprocessed sensory experience. \\
\hline
    \small\raggedright Feinman and Lake (2020) & Learning task-general representations with generative neuro-symbolic modeling & develop a generative neuro-symbolic (GNS) model of handwritten character concepts that uses the control flow of a probabilistic program, coupled with symbolic stroke primitives and a symbolic image renderer, to represent the causal and compositional processes by which characters are formed. \\
\hline
    \small\raggedright Feng et al. (2022) & Neuro-symbolic natural logic with introspective revision for natural language inference & Presents a NeSy framework to integrate natural logic with NNs to do natural language inference. They use RL to help guide this natural logic. Overall this improves the intuitive inference understandability of humans. \\
\hline
    \small\raggedright Gong et al. (2024) & Neuro-Symbolic Embedding for Short and Effective Feature Selection via Autoregressive Generation & Proposes a neuro-symbolic framework that models feature selection as an autoregressive token generation problem, learning embeddings for short and effective feature subsets with superior predictive performance compared to classical baselines. \\
\hline
    \small\raggedright Hakim et al. (2025) & ANSR-DT: An Adaptive Neuro-Symbolic Learning and Reasoning Framework for Digital Twins & ANSR-DT presents an adaptive neuro-symbolic framework integrating deep learning (CNN-LSTM) and symbolic reasoning (Prolog) for digital twins. The system generates interpretable insights, learns rules from data, and provides knowledge graph visualizations to support decision-making in industrial scenarios. \\
\hline
    \small\raggedright Han and Srivastava (2024) & An empirical evaluation of neural and neuro-symbolic approaches to real-time multimodal complex event detection & Description not available. \\
\hline
    \small\raggedright Princis et al. (2024) & Enhancing SQL Query Generation with Neurosymbolic Reasoning & Find a way to use language models to generate SQL queries. They introduce a new tool called Xander which helps \\
\hline
    \small\raggedright Ho Fung et al. (2024) & SymbolNet: Neural Symbolic Regression with Adaptive Dynamic Pruning & SymbolNet combines neural networks with symbolic regression through adaptive pruning to extract interpretable mathematical expressions while achieving high model compression. The method uses a custom training loop with threshold-based pruning to identify and eliminate less important network connections, enabling recovery of human-readable symbolic formulas from trained models without sacrificing predictive accuracy. \\
\hline
    \small\raggedright Howard et al. (2023) & Neurocomparatives: Neuro-symbolic distillation of comparative knowledge & This paper presents NeuroComparatives, a dataset of 8.8M comparative commonsense statements (e.g., "cats are typically smaller than dogs") generated using constrained beam search with GPT-2 and filtered using a discriminator trained on human annotations. The dataset aims to provide high-validity comparative knowledge for commonsense reasoning tasks. \\
\hline
    \small\raggedright Hu et al. (2023) & A multi-grained self-interpretable symbolic-neural model for single/multi-labeled text classification & propose a Symbolic-Neural model that can learn to explicitly predict class labels of text spans from a constituency tree without requiring any access to spanlevel gold labels \\
\hline
    \small\raggedright Ishay et al. (2024) & Think before You Simulate: Symbolic Reasoning to Orchestrate Neural Computation for Counterfactual Question Answering & The paper discusses a novel approach to do causal and temporal reasoning in video particularly for counterfactual reasoning. They make use of a causal graph and ASP to coordinate between perception and simulation modules. \\
\hline
    \small\raggedright Maene et al. (2024) & KLay: Accelerating Arithmetic Circuits for Neurosymbolic AI & The paper presents KLay, a scalable data structure and layerization algorithm that accelerates arithmetic circuit computation in neurosymbolic AI systems, enabling efficient forward and backward passes. It demonstrates near-linear runtime scaling across five orders of magnitude, significantly outperforming baseline methods on logic-based inference tasks. \\
\hline
    \small\raggedright Ying et al. (2025) & A Neural Symbolic Model for Space Physics & PhyE2E presents an end-to-end transformer-based framework for physics-informed symbolic regression that incorporates dimensional analysis as an inductive bias through oracle-guided divide-and-conquer strategies combined with MCTS refinement. The method achieves state-of-the-art performance on the Feynman equations benchmark, improving symbolic accuracy by 10-29\% over existing methods like PySR, uDSR, TPSR, and LaSR. \\
\hline
    \small\raggedright Kricheli et al. (2024) & Error Detection and Constraint Recovery in Hierarchical Multi-Label Classification without Prior Knowledge & his paper presents an approach that uses Error Detection Rules (EDR) to learn the failure modes of machine learning models, relaxing the common assumption that hierarchical constraints must exist beforehand. These learned rules are effective at detecting classifier errors and can be leveraged as constraints for Hierarchical Multi-label Classification (HMC), allowing for the recovery of explainable constraints even when they are not provided initially. \\
\hline
    \small\raggedright Ma et al. (2020) & Knowledge-driven Data Construction for Zero-shot Evaluation in Commonsense Question Answering & vary the set of language models, training regimes, knowledge sources, and data generation strategies, and measure their impact across tasks. Extending on prior work, we devise and compare four constrained distractor-sampling strategies \\
\hline
    \small\raggedright Katz et al. (2021) & Tunable Neural Encoding of a Symbolic Robotic Manipulation Algorithm & present a neurocomputational controller for robotic manipulation based on the recently developed “neural virtual machine” (NVM). Authors program the NVM with a symbolic algorithm that solves blocks-world restacking problems, and execute it in a robotic simulation environment \\
\hline
    \small\raggedright Kelly et al. (2023) & There and back again: extracting formal domains for controllable neurosymbolic story authoring & Demonstrate that languagemodels can be used to author narrative planning domainsfrom natural language stories with minimal human intervention. Second, authors explore the reverse, demonstrating that we can use logical story domains and plans to produce storiesthat respect the narrative commitments of the planne \\
\hline
    \small\raggedright Kohaut et al. (2024) & Probabilistic Mission Design in Neuro-Symbolic Systems & Authors propose Probabilistic Mission Design (ProMis), a novel neuro-symbolic approach to navigating UAS within legal frameworks. ProMis is an interpretable and adaptable system architecture that links uncertain geospatial data and noisy perception with declarative, Hybrid Probabilistic Logic Programs (HPLP) to reason over the agent’s state space and its legality \\
\hline
    \small\raggedright Kohaut et al. (2024) & The Constitutional Filter & introduces an approach for Bayesian estimation of agents expected to comply with a human-interpretable neuro-symbolic model we call its Constitution. Hence, autors present the Constitutional Filter (CoFi), leading to improved tracking of agents by leveraging expert knowledge, incorporating deep learning architectures, and accounting for environmental uncertainties \\
\hline
    \small\raggedright Kouvaros and Botoeva (2024) & Formal verification of parameterised neural-symbolic multi-agent systems & This paper presents VENMAS, a formal verification toolkit for multi-agent systems with neural components, enabling bounded temporal logic verification of parameterized neural-symbolic agents. The work extends existing verification methods to handle systems with an arbitrary number of homogeneous agents using symbolic abstraction techniques. \\
\hline
    \small\raggedright Kulmanov et al. (2021) & Semantic similarity and machine learning with ontologies & provide an overview over the methods that use ontologies to compute similarity and incorporate them in machine learning methods; in particular, the authors outline how semantic similarity measures and ontology embeddings can exploit the background knowledge in ontologies and how ontologies can provide constraints that improve machine learning models. \\
\hline
    \small\raggedright Kulmanov et al. (2024) & Protein function prediction as approximate semantic entailment & The Gene Ontology (GO) is a formal, axiomatic theory with over 100,000 axioms that describe the molecular functions, biological processes and cellular locations of proteins in three subontologies. Developed DeepGO-SE, a method that predicts GO functions from protein sequences using a pretrained large language model. DeepGO-SE generates multiple approximate models of GO, and a neural network predicts the truth values of statements about protein functions in these approximate models. \\
\hline
    \small\raggedright Lee et al. (2024) & Diakop: Dialogue-based knowledge-oriented programming for neural-symbolic knowledge base question answering & present Dialogue-based Knowledge-oriented Programming system (DiaKoP), a system with a chat interface designed for multi-turn knowledge base question answering (KBQA). DiaKoP enables users to decompose complex questions into multiple simpler follow-up questions and interact with the system to obtain answers. \\
\hline
    \small\raggedright Li et al. (2020) & Closed loop neural-symbolic learning via integrating neural perception, grammar parsing, and symbolic reasoning & This paper proposes a Neural-Grammar-Symbolic (NGS) model that improves upon inefficient reinforcement learning approaches by using a grammar model to bridge neural perception and symbolic reasoning, along with a novel back-search algorithm to learn from incorrect predictions. The experiments, conducted on handwritten formula recognition and visual question answering tasks, demonstrate that this method significantly outperforms reinforcement learning models in terms of performance, convergence speed, and data efficiency. \\
\hline
    \small\raggedright Li et al. (2022) & AdaLoGN: Adaptive Logic Graph Network for Reasoning-Based Machine Reading Comprehension & present a neural-symbolic approach which, to predict an answer, passes messages over a graph representing logical relations between text units. \\
\hline
    \small\raggedright Li et al. (2023) & Scallop: A language for neurosymbolic programming & present Scallop, a language which combines the benefits of deep learning and logical reasoning. Scallop enables users to write a wide range of neurosymbolic applications and train them in a data- and compute-efficient manner. It achieves these goals through three key features: 1) a flexible symbolic representation that is based on the relational data model; 2) a declarative logic programming language that is based on Datalog and supports recursion, aggregation, and negation; and 3) a framework for automatic and efficient differentiable reasoning that is based on the theory of provenance semirings. \\
\hline
    \small\raggedright Li et al. (2024) & Relational Programming with Foundation Models & implement VIEIRA by extending the SCALLOP compiler with a foreign interface that supports foundation models as plugins. We implement plugins for 12 foundation models including GPT, CLIP, and SAM. We evaluate VIEIRA on 9 challenging tasks that span language, vision, and structured and vector databases. Our evaluation shows that programs in VIEIRA are concise, can incorporate modern foundation models, and have comparable or better accuracy than competitive baselines. \\
\hline
    \small\raggedright Murphy et al. (2024) & Autoformalizing Euclidean Geometry & use theorem provers to fill in such diagrammatic information automatically, so that the LLM only needs to autoformalize the explicit textual steps, making it easier for the model. \\
\hline
    \small\raggedright Manginas et al. (2024) & NeSyA: Neurosymbolic Automata & how that symbolic automata can be integrated with neural-based perception, under probabilistic semantics towards an end-to-end differentiable model. Their proposed hybrid model, termed NESYA (Neuro Symbolic Automata) is shown to either scale or perform more accurately than previous NeSy systems in a synthetic benchmark and to provide benefits in terms of generalization compared to purely neural systems in a realworld event recognition task \\
\hline
    \small\raggedright Mejri et al. (2024) & LARS-VSA: A Vector Symbolic Architecture For Learning with Abstract Rules & The paper presents LARS-VSA, a neuro-symbolic framework leveraging hyperdimensional computing for abstract rule learning with compositional generalization. It introduces a high-dimensional attention mechanism and demonstrates superior generalization, efficiency, and accuracy over contemporary neural and neuro-symbolic baselines on multiple relational reasoning and math tasks. \\
\hline
    \small\raggedright Olausson et al. (2023) & (sic) LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers & investigate the validity of instead reformulating such tasks as modular neurosymbolic programming, which we call LINC: Logical Inference via Neurosymbolic Computation. In LINC, the LLM acts as a semantic parser, translating premises and conclusions from natural language to expressions in first-order logic. These expressions are then offloaded to an external theorem prover, which symbolically performs deductive inference. \\
\hline
    \small\raggedright Post et al. (2024) & Accelerating UMR adoption: Neuro-symbolic conversion from AMR-to-UMR with low supervision & The paper proposes a neuro-symbolic method for converting AMR (Abstract Meaning Representation) roles to UMR (Uniform Meaning Representation) roles by integrating animacy parsing and logic rules with a neural network. The approach addresses non-deterministic role mappings with minimal human supervision, achieving 75.81\% accuracy compared to a 62.35\% baseline neural network. \\
\hline
    \small\raggedright Premsri and Kordjamshidi (2024) & Neuro-symbolic Training for Reasoning over Spatial Language & This paper presents SpaRTUNQChain, a neuro-symbolic framework that combines BERT embeddings with spatial logic constraints for question-answering over spatial language. The system uses the DomiKnowS framework to enforce compositional reasoning rules during training, improving accuracy on spatial reasoning benchmarks. \\
\hline
    \small\raggedright Quan et al. (2025) & Peirce: Unifying material and formal reasoning via llm-driven neuro-symbolic refinement & introduce PEIRCE, a neuro-symbolic framework designed to unify material and formal inference through an iterative conjecture–criticism process. \\
\hline
    \small\raggedright Roig Vilamala et al. (2023) & DeepProbCEP: A neuro-symbolic approach for complex event processing in adversarial settings & Introduces DeepProbCEP, a differentiable ProbLog + CNN hybrid that trains end‑to‑end from complex‑event labels. \\
\hline
    \small\raggedright Shao et al. (2025) & Abductive Learning for Neuro-Symbolic Grounded Imitation & Description not available. \\
\hline
    \small\raggedright Skryagin et al. (2024) & Scalable Neural-Probabilistic Answer Set Programming & This paper introduces Answer Set Networks (ASNs), a novel neural-symbolic solver based on Graph Neural Networks (GNNs) designed to overcome the high computational costs and CPU-bound nature of traditional Answer Set Programming (ASP) solvers. By translating ASP problems into a graph format that leverages GPU parallelization, ASNs outperform existing systems and enable new applications like fine-tuning Large Language Models (LLMs) with logic and solving large-scale drone navigation tasks. \\
\hline
    \small\raggedright Winters et al. (2021) & DeepStochLog: Neural Stochastic Logic Programming & This paper introduces DeepStochLog, a novel neural-symbolic framework that integrates neural networks into stochastic definite clause grammars (SDCGs) to define a probability distribution over possible derivations. The experimental evaluation shows this approach scales significantly better than methods based on neural probabilistic logic programs and achieves state-of-the-art results on several challenging neural-symbolic tasks. \\
\hline
    \small\raggedright Trinh et al. (2024) & Solving olympiad geometry without human demonstrations & AlphaGeometry is a neuro-symbolic system that uses a neural language model, trained from scratch on our large-scale synthetic data, to guide a symbolic deduction engine through infinite branching points in challenging problems. \\
\hline
    \small\raggedright Varsou (2024) & The DeepProbCEP system for neuro-symbolic complex event recognition & Description not available. \\
\hline
    \small\raggedright Wu et al. (2024) & LLM Meets Bounded Model Checking: Neuro-symbolic Loop Invariant Inference & proposes LaM4Inv, a neuro-symbolic framework that combines large language models (Llama-3-8B, GPT-3.5, GPT-4, GPT-4-Turbo) with bounded model checking (ESBMC + SMT solvers) to automatically infer loop invariants for C programs. It evaluates LaM4Inv on an expanded benchmark of 316 loop-invariant problems, showing large gains in the number of verified loops compared to classical invariant generators and recent LLM-based baselines. \\
\hline
    \small\raggedright Wu and Nakayama (2024) & MILE: Memory-Interactive Learning Engine for Neuro-Symbolic Solutions to Mathematical Problems & MILE is a neuro-symbolic math word problem solver that predicts formulas using a memory-based decoder instead of tree-structured decoding, letting it handle more complex computation graphs. It beats existing methods on Math23K by learning formulas as rules. \\
\hline
    \small\raggedright Kang et al. (2025) & Neuro-Conceptual Artificial Intelligence: Integrating OPM with Deep Learning to Enhance Question Answering Quality & introduceNeuro-ConceptualArtificial Intelligence(NCAI),aspecializationof the neuro-symbolicAI approach that integrates conceptual modeling usingObject-Process Methodology (OPM) ISO19450:2024with deeplearningtoenhancequestion-answering (QA)quality \\
\hline
    \small\raggedright Yang et al. (2023) & Learning to Solve Constraint Satisfaction Problems with Recurrent Transformer & Constraint satisfaction problems (CSPs) are about finding values of variables that satisfy the given constraints. We show that Transformer extended with recurrence is a viable approach to learning to solve CSPs in an end-to-end manner, having clear advantages over state-of-the-art methods such as Graph Neural Networks, SATNet, and some neuro-symbolic models. \\
\hline
    \small\raggedright Yang et al. (2023) & Injecting Logical Constraints into Neural Networks via Straight-Through Estimators & This paper introduces CL-STE, a method to inject discrete logical constraints into neural networks by representing the constraints as a loss function and using a Straight-Through Estimator (STE) to enable gradient-based optimization. By leveraging GPUs and avoiding heavy symbolic computation, this technique scales significantly better than existing neuro-symbolic methods and allows various architectures, like CNNs and GNNs, to learn from constraints with fewer or no labeled data. \\
\hline
    \small\raggedright Li et al. (2025) & Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning & we introduce a neuro-symbolic tactic generator that synergizes the mathematical intuition learned by LLMs with domain-specific insights encoded by symbolic methods. \\
\hline
\end{longtable}
\end{onecolumn}
